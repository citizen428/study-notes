#+TITLE: Introduction to Artificial Intelligence
#+OPTIONS: H:2 num:f toc:t \n:nil @:t ::t |:t
#+STYLE:  <link rel="stylesheet" type="text/css" href="style.css" />
#+org-export-html-style-include-default: nil

* Welcome to AI
** Intelligent Agents
*** agent
*** environment
*** agent receives environment state through sensors
*** agent affects environment's state through actuators
*** control policy: function that maps sensors to actuators
*** perception action cycle: loop of environment feedback to sensors, agent decision, actuator interaction
** Applications of AI
*** finance
*** robotics
*** games
*** medicine (diagnostic agents)
*** on the web (crawling agents)
** Terminology
*** fully observable
**** what the agent can sense at any time is sufficient to make the optimal decisions
**** example: card game with all cards on the table
**** sensor can see complete state of environment
*** partially observable
**** needs memory on the side of the agent to make an optimal decision
**** sensor can only see part of environment, but memorizing past measurements gives additional information of the state
*** deterministic
**** agent's actions uniquely determine outcome
**** example: chess
*** stochastic
**** element of randomness involved
**** example: games with dice, like backgammon
*** discrete
**** finitely many action choices
**** finitely many things to sense
**** example: chess - finitely many board positions
*** continuous
**** space of possible actions or things to sense may be infinite
**** example: darts - infinitely many angles and accelerations
*** benign
**** random environment (stochastic), but no objective of its own
**** example: weather
*** adversarial
**** example: games - harder to find good actions because of opponent
*** Quiz: Checkers
**** fully observable
**** deterministic
**** discrete
**** adversarial
*** Quiz: Poker
**** partially observable
**** stochastic
**** discrete
**** adversarial
*** Quiz: Robot Car
**** partially observable
**** stochastic
**** continuous
**** benign
** AI and Uncertainty
*** AI is a technique for uncertainty management in computer software
*** discipline of knowing what to do when you don't know what to do
*** reasons for uncertainty
**** sensor limits
**** adversaries
**** stochastic environments
**** laziness
**** ignorance
** Machine Translation
*** big success at Google
*** supports 50 languages
*** machine learning, using AI
*** collect text examples in 2 languages
*** finding most probable translations from snippets
* Problem Solving
** What is a problem?
*** Definition of a problem
**** =s=: state, =a=: action, =T=: true, =F=: false, =n=: cost
**** initial state
**** =actions(s) -> {a, a2, a3, ...}=
**** =result(s, a) -> s'â€‹=
**** =goaltest(s) -> T | F=
**** =pathcost(s->s->s) -> n=, implemented in terms of =stepcost(s, a, s') -> n=
** Example: Route Finding
*** state space = set of all states
*** ends of the paths (furthest explorations) are called frontier
*** part to the left (on the map): explored region
*** part to the right: unexplored region
** Tree Search
*** superimposes a search tree over the state space
*** family of functions with different choice functions
*** starts by initializing frontier to initial state
*** then goes into loop:
**** if frontier empty, no solution -> FAIL
**** otherwise choice function
*** pseudo code
#+BEGIN_EXAMPLE
  function tree-search(problem):
    frontier = { [initial] }
    loop:
      if frontier is empty: return FAIL
      path = remove-choice(frontier)
      s = path.end
      if GOALTEST(s): return path
      for a in p.Actions(s):
        Add [path+a -> Result(s,a)] to frontier
#+END_EXAMPLE
*** breadth-first search
**** could be called shortest-first search
**** takes shortest possible unconsidered path from the frontier
** Graph Search
*** avoids repeated paths of tree search
*** adds an explored set to keep tracks of already explored states
*** pseudo code
#+BEGIN_EXAMPLE
  function graph-search(problem):
    frontier = { [initial] }; explored = {}
    loop:
      if frontier is empty: return FAIL
      path = remove-choice(frontier)
      s = path.end; add to explored
      if s is a goal: return path
      for a in p.Actions(s):
        Add [path+a -> Result(s,a)] to frontier
        unless Result(s,a) in frontier+explored
#+END_EXAMPLE
** Uniform Cost Search
*** could be called cheapest-first search
** Search Comparison
*** breadth-first: expand shortest/shallowest path; optimal
*** cheapest-first: expand path with lowest total cost; optimal
*** depth-first: expand longest path; not optimal
*** storage requirements
**** breadth-first: 2^n nodes
**** cheapest-first: similar
**** depth-first: n nodes
**** lower savings when keeping track of exploring set
*** completeness
**** Will algorithm find a goal?
**** breath-first and cheapest first are complete
**** depth-first is incomplete
** More on Uniform Cost
*** most useful knowledge: estimate of distance between start state and goal
*** example f. route finding: straight line distance
*** greedy best-first search
**** first expands path closest to the goal according to estimate
**** accepts path that's longer than other paths (not optimal)
** A* Search
*** combines the best parts of greedy search (explores small number of nodes in many cases) and uniform cost search (guaranteed to find shortest path)
*** always expand path that has minimum value of the function =f = g + h=
**** =g(path)=: path cost
**** =h(path)= = =h(s)= = estimated distance to goal
*** minimizing =g= keeps path short
*** minimizing =h= keeps us focused on finding the goal
*** "best estimated total path cost first"
*** if =h(s) <= true cost=, A* finds the shortest path
*** =h= is optimistic (=h= should never overestimate distance to the goal)
** State Spaces
*** x/y coordinates in the plane
*** vacuum world
**** 2 physical states that robot vacuum cleaner can be in
**** each can be dirty or not
**** robot can be in either
**** =2 * 2 * 2 = 8 possible states=
*** more complicated vacuum world
**** power switch: on, off, sleep
**** dirt-sensing camera: on, off
**** brushes at 5 different heights
**** 10 positions
**** =3 * 2 * 5 * 2^10 * 10 = 307,200 states=
** Sliding Block Puzzle
*** goal state: numbers in order
*** starting state: random
*** heuristics
**** =h1=: # misplaces blocks
**** =h2=: sum(distances of blocks)
**** both are admissible heuristics (=h2= is always >= than =h1=), so A* with =h2= will always expand fewer paths
*** program that can come up with good heuristics
**** =h1= and =h2= can be derived from problem description
**** =h = max(h1, h2)=
*** "generating a relaxed problem"
**** relaxing problem constraints
** Problems with Search
*** problem-solving technology works when the domain is
**** fully observable
**** known
**** discrete
**** deterministic
**** static
** Note on Implementation
*** node: data structure with 4 fields
**** state field: state at end of path
**** action: action it took to get here
**** cost: total cost
**** parent: pointer to another node
*** linked list of nodes represents path
*** data structures dealing with nodes
**** frontier: priority queue / set
**** explored list: set
